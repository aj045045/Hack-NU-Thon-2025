{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-31T02:45:49.637405Z",
     "iopub.status.busy": "2025-03-31T02:45:49.637113Z",
     "iopub.status.idle": "2025-03-31T02:45:49.643780Z",
     "shell.execute_reply": "2025-03-31T02:45:49.642817Z",
     "shell.execute_reply.started": "2025-03-31T02:45:49.637372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bank-account-fraud-dataset-neurips-2022/Base.csv\n",
      "/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant IV.csv\n",
      "/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant V.csv\n",
      "/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant I.csv\n",
      "/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant III.csv\n",
      "/kaggle/input/bank-account-fraud-dataset-neurips-2022/Variant II.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# ----- Setup Logging -----\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ----- Step 1: Load Data -----\n",
    "def load_data(filepath):\n",
    "    logger.info(\"Loading data from %s\", filepath)\n",
    "    data = pd.read_csv(filepath)\n",
    "    logger.info(\"Data loaded. Shape: %s\", data.shape)\n",
    "    return data\n",
    "\n",
    "# ----- Step 2: Preprocess Data -----\n",
    "def preprocess_data(data):\n",
    "    logger.info(\"Preprocessing data\")\n",
    "    y = data['fraud_bool']\n",
    "    X = data.drop('fraud_bool', axis=1)\n",
    "\n",
    "    categorical_cols = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numerical_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train_proc, X_test_proc, y_train, y_test, preprocessor\n",
    "\n",
    "# ----- Step 3: Oversampling (Hybrid SMOTE + ADASYN) -----\n",
    "def oversample_data(X_train, y_train):\n",
    "    logger.info(\"Performing hybrid oversampling using SMOTE and ADASYN\")\n",
    "    hybrid_sampler = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = hybrid_sampler.fit_resample(X_train, y_train)\n",
    "    adasyn_sampler = ADASYN(random_state=42)\n",
    "    X_resampled, y_resampled = adasyn_sampler.fit_resample(X_resampled, y_resampled)\n",
    "    logger.info(\"Oversampling complete. New class distribution: %s\", Counter(y_resampled))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# ----- Step 4: Fit IsolationForest -----\n",
    "def fit_isolation_forest(X_train):\n",
    "    logger.info(\"Fitting IsolationForest\")\n",
    "    iso_forest = IsolationForest(random_state=42, contamination=0.1, n_jobs=-1)\n",
    "    iso_forest.fit(X_train)\n",
    "    logger.info(\"IsolationForest fitted.\")\n",
    "    return iso_forest\n",
    "\n",
    "# ----- Step 5: Compute Anomaly Scores -----\n",
    "def compute_anomaly_scores(iso_forest, X):\n",
    "    logger.info(\"Computing anomaly scores\")\n",
    "    anomaly_scores = iso_forest.decision_function(X)\n",
    "    return anomaly_scores\n",
    "\n",
    "# ----- Step 6: Augment Data with Anomaly Scores -----\n",
    "def augment_data_with_anomaly_scores(X, anomaly_scores, preprocessor):\n",
    "    logger.info(\"Augmenting data with anomaly scores\")\n",
    "    feature_names = preprocessor.get_feature_names_out().tolist()\n",
    "    X_aug = pd.DataFrame(X, columns=feature_names)\n",
    "    X_aug['anomaly_score'] = anomaly_scores\n",
    "    return X_aug\n",
    "\n",
    "# ----- Step 6: Train XGBoost with Grid Search -----\n",
    "def train_xgboost_in_batches(X_train, y_train, batch_size=50000):\n",
    "    logger.info(\"Training XGBoost in batches using CPU & GPU\")\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"random_state\": 42,\n",
    "        \"tree_method\": \"gpu_hist\",  \n",
    "        \"device\": \"cuda\",\n",
    "        \"max_depth\": 5,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"n_estimators\": 200,  \n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    num_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, len(X_train))\n",
    "        logger.info(f\"Training batch {i+1}/{num_batches}: Samples {start}-{end}\")\n",
    "\n",
    "        # Convert batch to DeviceQuantileDMatrix (faster on GPU)\n",
    "        dtrain = xgb.DeviceQuantileDMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "        model.fit(X_train[start:end], y_train[start:end], xgb_model=model.get_booster() if i > 0 else None)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ----- Step 7: Save Model -----\n",
    "def save_model(model, preprocessor, iso_forest, directory=\"saved_models\"):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    joblib.dump(model, os.path.join(directory, \"xgb_model.pkl\"))\n",
    "    joblib.dump(preprocessor, os.path.join(directory, \"preprocessor.pkl\"))\n",
    "    joblib.dump(iso_forest, os.path.join(directory, \"iso_forest.pkl\"))\n",
    "\n",
    "# ----- Step 8: Generate and Save Analytical Graphs -----\n",
    "def generate_graphs(model, X_test, y_test, directory=\"saved_graphs\"):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Confusion Matrix Heatmap\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(os.path.join(directory, \"confusion_matrix.png\"))\n",
    "    \n",
    "    # ROC Curve\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label='ROC Curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(directory, \"roc_curve.png\"))\n",
    "    \n",
    "    # Feature Importance\n",
    "    xgb.plot_importance(model)\n",
    "    plt.savefig(os.path.join(directory, \"feature_importance.png\"))\n",
    "    \n",
    "    # SHAP Analysis\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.savefig(os.path.join(directory, \"shap_summary.png\"))\n",
    "\n",
    "# ----- Main Execution -----\n",
    "def main():\n",
    "    filepath = \"/kaggle/input/bank-account-fraud-dataset-neurips-2022/Base.csv\"\n",
    "    data = load_data(filepath)\n",
    "    X_train, X_test, y_train, y_test, preprocessor = preprocess_data(data)\n",
    "    X_train_resampled, y_train_resampled = oversample_data(X_train, y_train)\n",
    "    iso_forest = fit_isolation_forest(X_train_resampled)\n",
    "    anomaly_scores_train = compute_anomaly_scores(iso_forest, X_train_resampled)\n",
    "    anomaly_scores_test = compute_anomaly_scores(iso_forest, X_test)\n",
    "    X_train_aug = augment_data_with_anomaly_scores(X_train_resampled, anomaly_scores_train, preprocessor)\n",
    "    X_test_aug = augment_data_with_anomaly_scores(X_test, anomaly_scores_test, preprocessor)          \n",
    "    best_model = train_xgboost_with_grid_search(X_train_resampled, y_train_resampled)\n",
    "    save_model(best_model, preprocessor, iso_forest)\n",
    "    generate_graphs(best_model, X_test, y_test)\n",
    "    logger.info(\"Model training complete and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model with more efficient data consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# import logging\n",
    "# import os\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from collections import Counter\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "# import cudf  # GPU-accelerated pandas\n",
    "\n",
    "# # ----- Setup Logging -----\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # ----- Step 1: Load Data -----\n",
    "# def load_data(filepath):\n",
    "#     logger.info(f\"Loading data from {filepath}\")\n",
    "#     data = cudf.read_csv(filepath)\n",
    "#     logger.info(f\"Data loaded. Shape: {data.shape}\")\n",
    "#     return data\n",
    "\n",
    "# # ----- Step 2: Preprocess Data -----\n",
    "# def preprocess_data(data):\n",
    "#     logger.info(\"Preprocessing data\")\n",
    "#     y = data['fraud_bool']\n",
    "#     X = data.drop('fraud_bool', axis=1)\n",
    "\n",
    "#     categorical_cols = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "#     numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", StandardScaler(), numerical_cols),\n",
    "#             (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
    "#         ],\n",
    "#         remainder=\"passthrough\"\n",
    "#     )\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=42)\n",
    "#     X_train_proc = preprocessor.fit_transform(X_train)\n",
    "#     X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "#     logger.info(\"Preprocessing complete.\")\n",
    "#     return X_train_proc, X_test_proc, y_train, y_test, preprocessor\n",
    "\n",
    "# # ----- Step 3: Oversampling -----\n",
    "# def oversample_data(X_train, y_train):\n",
    "#     logger.info(\"Performing hybrid oversampling using SMOTE and ADASYN\")\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "#     adasyn = ADASYN(random_state=42)\n",
    "#     X_resampled, y_resampled = adasyn.fit_resample(X_resampled, y_resampled)\n",
    "#     logger.info(f\"Oversampling complete. New class distribution: {Counter(y_resampled)}\")\n",
    "#     return X_resampled, y_resampled\n",
    "\n",
    "# # ----- Step 4: Fit IsolationForest -----\n",
    "# def fit_isolation_forest(X_train):\n",
    "#     logger.info(\"Fitting IsolationForest\")\n",
    "#     iso_forest = IsolationForest(random_state=42, contamination=0.1, n_jobs=-1)\n",
    "#     iso_forest.fit(X_train)\n",
    "#     logger.info(\"IsolationForest fitted.\")\n",
    "#     return iso_forest\n",
    "\n",
    "# # ----- Step 5: Compute Anomaly Scores -----\n",
    "# def compute_anomaly_scores(iso_forest, X):\n",
    "#     logger.info(\"Computing anomaly scores\")\n",
    "#     return iso_forest.decision_function(X)\n",
    "\n",
    "# # ----- Step 6: Augment Data with Anomaly Scores -----\n",
    "# def augment_data_with_anomaly_scores(X, anomaly_scores, preprocessor):\n",
    "#     logger.info(\"Augmenting data with anomaly scores\")\n",
    "#     feature_names = preprocessor.get_feature_names_out().tolist()\n",
    "#     X_aug = pd.DataFrame(X, columns=feature_names)\n",
    "#     X_aug['anomaly_score'] = anomaly_scores\n",
    "#     return X_aug\n",
    "\n",
    "# # ----- Step 7: Train XGBoost on GPU -----\n",
    "# def train_xgboost(X_train, y_train):\n",
    "#     logger.info(\"Training XGBoost using GPU\")\n",
    "#     dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "#     params = {\n",
    "#         \"objective\": \"binary:logistic\",\n",
    "#         \"eval_metric\": \"logloss\",\n",
    "#         \"random_state\": 42,\n",
    "#         \"tree_method\": \"hist\",  # Corrected method\n",
    "#         \"device\": \"cuda\",\n",
    "#         \"max_depth\": 5,\n",
    "#         \"learning_rate\": 0.1,\n",
    "#         \"subsample\": 0.8,\n",
    "#         \"n_estimators\": 200,\n",
    "#         \"n_jobs\": -1\n",
    "#     }\n",
    "#     model = xgb.train(params, dtrain, num_boost_round=200)\n",
    "#     logger.info(\"XGBoost training complete.\")\n",
    "#     return model\n",
    "\n",
    "# # ----- Step 8: Save Model -----\n",
    "# def save_model(model, preprocessor, directory=\"saved_models\"):\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "#     joblib.dump(model, os.path.join(directory, \"xgb_model.pkl\"))\n",
    "#     joblib.dump(preprocessor, os.path.join(directory, \"preprocessor.pkl\"))\n",
    "#     logger.info(\"Model and preprocessor saved.\")\n",
    "\n",
    "# # ----- Step 9: Generate Graphs -----\n",
    "# def generate_graphs(model, X_test, y_test, preprocessor, directory=\"saved_graphs\"):\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "    \n",
    "#     logger.info(\"Generating graphs and SHAP analysis\")\n",
    "#     y_pred_prob = model.predict(xgb.DMatrix(X_test))\n",
    "#     y_pred = y_pred_prob > 0.5\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "#     plt.xlabel(\"Predicted Label\")\n",
    "#     plt.ylabel(\"True Label\")\n",
    "#     plt.savefig(os.path.join(directory, \"confusion_matrix.png\"))\n",
    "#     plt.close()\n",
    "#     logger.info(\"Confusion matrix saved.\")\n",
    "    \n",
    "#     fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.plot(fpr, tpr, label='ROC Curve')\n",
    "#     plt.title(\"ROC Curve\")\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.legend()\n",
    "#     plt.savefig(os.path.join(directory, \"roc_curve.png\"))\n",
    "#     plt.close()\n",
    "#     logger.info(\"ROC curve saved.\")\n",
    "    \n",
    "#     explainer = shap.Explainer(model)\n",
    "#     shap_values = explainer(X_test)\n",
    "#     shap_df = pd.DataFrame(shap_values.values, columns=preprocessor.get_feature_names_out().tolist())\n",
    "#     shap_df.to_csv(os.path.join(directory, \"shap_analysis.csv\"), index=False)\n",
    "#     logger.info(\"SHAP analysis saved.\")\n",
    "\n",
    "# # ----- Main Execution -----\n",
    "# def main():\n",
    "#     filepath = \"/kaggle/input/bank-account-fraud-dataset-neurips-2022/Base.csv\"\n",
    "#     data = load_data(filepath)\n",
    "#     X_train, X_test, y_train, y_test, preprocessor = preprocess_data(data)\n",
    "#     X_train_resampled, y_train_resampled = oversample_data(X_train, y_train)\n",
    "#     iso_forest = fit_isolation_forest(X_train_resampled)\n",
    "#     X_train_aug = augment_data_with_anomaly_scores(X_train_resampled, compute_anomaly_scores(iso_forest, X_train_resampled), preprocessor)\n",
    "#     X_test_aug = augment_data_with_anomaly_scores(X_test, compute_anomaly_scores(iso_forest, X_test), preprocessor)\n",
    "#     model = train_xgboost(X_train_resampled, y_train_resampled)\n",
    "#     save_model(model, preprocessor)\n",
    "#     generate_graphs(model, X_test, y_test, preprocessor)\n",
    "#     logger.info(\"Model training and analysis complete.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T02:13:08.248179Z",
     "iopub.status.busy": "2025-03-31T02:13:08.247968Z",
     "iopub.status.idle": "2025-03-31T02:13:11.670010Z",
     "shell.execute_reply": "2025-03-31T02:13:11.669163Z",
     "shell.execute_reply.started": "2025-03-31T02:13:08.248159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# file_path = \"/kaggle/input/bank-account-fraud-dataset-neurips-2022/Base.csv\"  # Replace with the actual file path\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # Check for missing values\n",
    "# print(data.isnull().sum())\n",
    "# print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T02:13:11.671041Z",
     "iopub.status.busy": "2025-03-31T02:13:11.670834Z",
     "iopub.status.idle": "2025-03-31T02:13:14.783414Z",
     "shell.execute_reply": "2025-03-31T02:13:14.782411Z",
     "shell.execute_reply.started": "2025-03-31T02:13:11.671023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# def preprocess_data(data):\n",
    "#     y = data['fraud_bool']\n",
    "#     X = data.drop('fraud_bool', axis=1)\n",
    "\n",
    "#     categorical_cols = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "#     numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", StandardScaler(), numerical_cols),\n",
    "#             (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
    "#         ],\n",
    "#         remainder=\"passthrough\"\n",
    "#     )\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "#     X_train_proc = preprocessor.fit_transform(X_train)\n",
    "#     X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "#     return X_train_proc, X_test_proc, y_train, y_test, preprocessor\n",
    "\n",
    "# X_train, X_test, y_train, y_test, preprocessor = preprocess_data(data)\n",
    "# print(\"Preprocessing done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T02:13:23.661380Z",
     "iopub.status.busy": "2025-03-31T02:13:23.661058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# def apply_oversampling(X_train, y_train):\n",
    "#     tqdm.pandas(desc=\"Applying SMOTE + ADASYN\")\n",
    "#     # smote = SMOTE(random_state=42)\n",
    "#     # X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "#     # adasyn = ADASYN(random_state=42)\n",
    "#     # X_resampled, y_resampled = adasyn.fit_resample(X_resampled, y_resampled)\n",
    "#     # logger.info(f\"Oversampling complete. New class distribution: {Counter(y_resampled)}\")\n",
    "#     # return X_resampled, y_resampled\n",
    "    \n",
    "#     # SMOTE + ENN\n",
    "#     smote_adasyn = SMOTEENN(sampling_strategy=\"auto\")\n",
    "#     X_train_resampled, y_train_resampled = smote_adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "#     # ADASYN\n",
    "#     adasyn = ADASYN(sampling_strategy=\"auto\", random_state=42)\n",
    "#     X_train_final, y_train_final = adasyn.fit_resample(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "#     return X_train_final, y_train_final\n",
    "\n",
    "# X_train_bal, y_train_bal = apply_oversampling(X_train, y_train)\n",
    "# print(f\"Oversampling done! New shape: {X_train_bal.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# def fit_isolation_forest(X_train):\n",
    "#     print(\"Training Isolation Forest...\")\n",
    "#     iso_forest = IsolationForest(n_estimators=100, contamination=0.02, random_state=42, n_jobs=-1)\n",
    "#     iso_forest.fit(X_train)\n",
    "    \n",
    "#     anomaly_scores = iso_forest.decision_function(X_train)  # Get anomaly scores\n",
    "#     return iso_forest, anomaly_scores\n",
    "\n",
    "# iso_forest_model, anomaly_scores = fit_isolation_forest(X_train_bal)\n",
    "# print(\"Isolation Forest training done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Add anomaly score as a new feature\n",
    "# X_train_bal = np.hstack((X_train_bal, anomaly_scores.reshape(-1, 1)))\n",
    "# X_test = np.hstack((X_test, iso_forest_model.decision_function(X_test).reshape(-1, 1)))\n",
    "\n",
    "# print(\"Anomaly scores added to dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# def train_xgboost(X_train, y_train):\n",
    "#     print(\"Training XGBoost with Grid Search...\")\n",
    "\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [100, 200],\n",
    "#         'max_depth': [3, 5, 7],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'subsample': [0.8, 1.0],\n",
    "#         'colsample_bytree': [0.8, 1.0]\n",
    "#     }\n",
    "\n",
    "#     xgb = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "#     grid_search = GridSearchCV(xgb, param_grid, scoring=\"f1\", cv=3, verbose=2, n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "#     return grid_search.best_estimator_\n",
    "\n",
    "# xgb_model = train_xgboost(X_train_bal, y_train_bal)\n",
    "# print(\"XGBoost training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# joblib.dump(xgb_model, \"fraud_detection_xgb.pkl\")\n",
    "# print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Predict on test data\n",
    "# y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# # Generate Confusion Matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Fraud\", \"Fraud\"], yticklabels=[\"No Fraud\", \"Fraud\"])\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# y_pred_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# plt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.2f}\")\n",
    "# plt.plot([0, 1], [0, 1], \"r--\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.title(\"ROC Curve\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shap\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# explainer = shap.Explainer(xgb_model)\n",
    "# shap_values = explainer(X_test)\n",
    "\n",
    "# # Convert SHAP values to DataFrame\n",
    "# shap_df = pd.DataFrame(shap_values.values, columns=[f\"Feature_{i}\" for i in range(X_test.shape[1])])\n",
    "# shap_df.to_csv(\"shap_analysis.csv\", index=False)\n",
    "\n",
    "# print(\"SHAP values saved to shap_analysis.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2673949,
     "sourceId": 7082010,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
